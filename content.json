{"meta":{"title":"To be a hacker!","subtitle":"hacktf","description":null,"author":"hacktf","url":"http://www.hacktf.com"},"pages":[{"title":"Categories","date":"2017-06-20T14:50:52.000Z","updated":"2017-06-20T14:50:52.000Z","comments":true,"path":"categories/index.html","permalink":"http://www.hacktf.com/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2017-06-20T14:50:52.000Z","updated":"2017-06-20T14:50:52.000Z","comments":true,"path":"tags/index.html","permalink":"http://www.hacktf.com/tags/index.html","excerpt":"","text":""},{"title":"About","date":"2017-06-20T14:50:52.000Z","updated":"2017-06-20T14:50:52.000Z","comments":true,"path":"about/index.html","permalink":"http://www.hacktf.com/about/index.html","excerpt":"","text":""}],"posts":[{"title":"Stanford TF基本概念","slug":"stanford-tensorflow-class-concept","date":"2017-09-02T02:56:11.000Z","updated":"2017-09-02T15:57:18.000Z","comments":true,"path":"2017/09/02/stanford-tensorflow-class-concept/","link":"","permalink":"http://www.hacktf.com/2017/09/02/stanford-tensorflow-class-concept/","excerpt":"","text":"Stanford大学Tensorflow课程基本概念。主要是包括有: Graph在Graph中，定义多个Operation. 常量Constant特别需要知道的是Constant和Variable的区别，Constant表示常量，其值是存储在GraphDef中，如定义 12345import tensorflow as tfx = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]], name='x')with tf.Session() as sess: print sess.graph.as_graph_def() 123456789101112131415161718192021222324252627node &#123; name: \"x\" op: \"Const\" attr &#123; key: \"dtype\" value &#123; type: DT_INT32 &#125; &#125; attr &#123; key: \"value\" value &#123; tensor &#123; dtype: DT_INT32 tensor_shape &#123; dim &#123; size: 3 &#125; dim &#123; size: 3 &#125; &#125; tensor_content: \"\\001\\000\\000\\000\\002\\000\\000\\000\\003\\000\\000\\000\\004\\000\\000\\000\\005\\000\\000\\000\\006\\000\\000\\000\\007\\000\\000\\000\\010\\000\\000\\000\\t\\000\\000\\000\" &#125; &#125; &#125;&#125; 变量Variable使用tf.constant来定义常量，使用tf.Variable来定义变量，是存在着很大的不同，原因是: tf.constant是一个operation, tf.Variable是一个类. 参考Variable类里面的说明 A variable maintains state in the graph across calls to ‘run()’. 即在各个run运行过程中状态是一致的，上个run过程中更改了该状态，下一个run过程中是可以读取最新的更新完之后的状态。 在初始化的时候，会传入该变量的初始化值，在初始化值中就已经包括有type和shape信息，传入初始化值之后就固定了，后面只能通过assign等方法来更改其中的值。 在Graph运行之前，Variable必须要显式地初始化掉，如调用sess.run(v.initializer). 最常见的方式是新建一个op，init_op = tf.global_variables_initializer(), 然后sess.run(init_op) 在tf.Variable这个类中，定义了好几个方法: 方法 含义 initializer 初始化 value() 读操作，读取值 assign() 写操作，更改值 assign_add() 写操作，增加值 assign_sub() 写操作，减少值 eval() 读操作，评估得到的变量值 SessionOperation现有的Operation列表有： Category Examples Element-wise mathematical operations Add, Sub, Mul, Div, Exp, Log, Greater, Less, Equal, … Array operations Concat, Slice, Split, Constant, Rank, Shape, Shuffle, … Matrix operations MatMul, MatrixInverse, MatrixDeterminant, … Stateful operations Variable, Assign, AssignAdd, … Neural network building blocks SoftMax, Sigmoid, ReLU, Convolution2D, MaxPool, … Checkpointing operations Save, Store Queue and synchronization operations Enqueue, Dequeue, MutexAcquire, MutexRelease Control flow operations Merge, Switch, Enter, Leave, NextIteration Tensorboard为了方便学习TF， TF提供了不错的tensorboard工具，进行可视化地学习TF. 简单的学习Tensorboard的步骤为: 1234567891011import tensorflow as tfa = tf.constant(2)b = tf.constant(3)x = tf.add(a, b)with tf.Session() as sess: writer = tf.summary.FileWriter('/tmp/tf', sess.graph) print sess.run(x) writer.close() 使用Tensorflow，然后执行tensorboard --logdir=/tmp/tf, 然后在浏览器里面输入 http://localhost:6006 之后，就可以得到输出结果为 Optimizer参考linear regression的例子，使用GradientDescentOptimizer进行优化loss，不断迭代，最后得到一个w和b的值。 123456789101112131415161718192021X = tf.placeholder(tf.float32, name='X')Y = tf.placeholder(tf.float32, name='Y')w = tf.Variable(0.0, name='weights')b = tf.Variable(0.0, name='bias')Y_predicted = X * w + bloss = tf.square(Y - Y_predicted, name='loss')optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for i in range(100): total_loss = 0.0 for x, y in data: _, loss = sess.run([optimizer, loss], feed_dict=&#123;X: x, Y: y&#125;) total_loss += loss print('epoch &#123;0&#125;: &#123;1&#125;'.format(i, total_loss/n_samples)) w, b = sess.run([w, b]) 上面构建的模型，使用Tensorboard进行查看得到的是如下图所示： 说明： bias, weights是在Tensorflow模型中定义的变量(Variable). 而gradients是由tensorflow框架基于optimizer自动生成。 同时还有一个GradientDescent，依赖于bias, weights, gradients Minimize实现细节使用了GradientDescentOptimizer意味着使用梯度下降的方式进行更新，在Tensorflow中使用auto differentiation 来自动更新w和b的值，然后来最小化loss。 当然也可以基于Optimizer自己进行gradient计算。在上面的代码中，直接调用optimizer.minimize(loss),在这个调用背后做了计算梯度并且将梯度应用到各个变量上去(computing the gradients and applying them to the variables)。若想在计算梯度和将梯度应用到变量去中间加入一个逻辑，可以进行如下操作： 调用compute_gradients()来计算梯度 对gradients进行处理 调用apply_gradients()来将梯度应用到变量上去 1234567891011# create an optimizeroptimizer = GradientDescentOptimizer(learning_rate=0.01)# compute the gradients for a list of variablesgrads_and_vars = optimizer.compute_gradients(loss, &lt;list of varibles&gt;)# grads_and_vars is a list of tuples (gradient, variable). do whatever you needcapped_grads_and_vars = [(MyCapper(gv[0]), gv[1]) for gv in grads_and_vars]# ask the optimizer to apply the capped gradientsoptimizer.apply_gradients(capped_grads_and_vars) 梯度计算细节虽然optimizer可以自动计算graph的梯度，但是对于新的optimizer的作者或者高级用户还是希望能够调用底层的接口来进行梯度计算。 1234567tf.gradients(ys, xs, grad_ys=None, name='gradients', colocate_gradients_with_ops=False, gate_gradients=False, aggregation_method=None) 使用上面的函数来计算针对xs对ys的符号偏导。 TODO: 增加更多的梯度计算细节 Optimizer列表当前在Tensorflow中已经提供了一系列的Optimizer，总结如下： 12345678910tf.train.GradientDescentOptimizertf.train.AdadeltaOptimizertf.train.AdagradOptimizertf.train.AdagradDAOptimizertf.train.MomentumOptimizertf.train.AdamOptimizertf.train.FtrlOptimizertf.train.ProximalGradientDescentOptimizertf.train.ProximalAdagradOptimizertf.train.RMSPropOptimizer","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://www.hacktf.com/categories/深度学习/"}],"tags":[{"name":"CS20SI","slug":"CS20SI","permalink":"http://www.hacktf.com/tags/CS20SI/"},{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://www.hacktf.com/tags/Tensorflow/"}]},{"title":"Stanford TF课程总览","slug":"stanford-tensorflow-class-index","date":"2017-09-02T02:28:35.000Z","updated":"2017-09-02T06:40:57.000Z","comments":true,"path":"2017/09/02/stanford-tensorflow-class-index/","link":"","permalink":"http://www.hacktf.com/2017/09/02/stanford-tensorflow-class-index/","excerpt":"","text":"《CS 20SI: Tensorflow for Deep Learning Research》是Stanford大学开的一门基于Tensorflow的课程，在这门课程中将讲述如何来使用Tensorflow这个工具进行深度学习研究。 注重讲述Tensorflow的图计算模型(Graphical computational model)，并且如何搭建使用深度学习项目的模型。通过该门课程的学习，可以搭建不同复杂度的模型（从简单的linear/logistic regression模型到复杂的CNN/RNN等模型）去解决word embeddings, translation, OCR等任务。 Tensorflow基础该部分主要讲述Graph, Session, Tensorboard, Operations, Optimizer等基本概念，然后深入到模型层级来展开如何来构建模型，如何训练模型 基本概念在Tensorflow中，Graph和Session是非常基础的概念，在Python接口中暴露出来的分别是tf.Graph和tf.Session，对Graph和Session简单定义为: 12* Graph: A graph defines the computation. 仅仅只是定义了计算，不进行任何的计算，也不会保存任何的值。* Session: A session allows to execute graphs or part of graphs. 在Session中分配所需要的资源，并且保存结果和变量的中间状态 详见Stanford TF基本概念 构建模型以word2vec为例来说明如何构建TF模型 训练模型深度学习实践包括使用Tensorflow来进行CNN, RNN等深度学习模型实践 CNNRNN强化训练 Reinforcement Learning","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://www.hacktf.com/categories/深度学习/"}],"tags":[{"name":"CS20SI","slug":"CS20SI","permalink":"http://www.hacktf.com/tags/CS20SI/"},{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://www.hacktf.com/tags/Tensorflow/"}]}]}